{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa988391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_73857/3481251351.py:57: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total songs in dataset: ../data/train_with_additional.csv: 290681\n",
      "\n",
      "Label distribution in processed dataset:\n",
      "categories\n",
      "[None]                                                    46387\n",
      "[Emotions, Body Parts]                                    19824\n",
      "[Emotions]                                                18594\n",
      "[Emotions, Body Parts, Family]                            11684\n",
      "[Emotions, Family]                                        10439\n",
      "                                                          ...  \n",
      "[Animals, Clothing, Food, Emotions]                          39\n",
      "[Animals, Clothing, Food, Emotions, Family, Transport]       38\n",
      "[Animals, Clothing, Food, Transport]                         37\n",
      "[Animals, Clothing, Food, Family, Sports]                    30\n",
      "[Animals, Clothing, Food, Sports]                            28\n",
      "Name: count, Length: 256, dtype: int64\n",
      "Number of songs with categories (non-['None']): 290681\n",
      "Number of songs with no categories (['None']): 0\n",
      "Processed dataset saved to ../data/processed_train_dataset.csv\n",
      "Total songs in processed dataset: 290681\n",
      "Loading test data...\n",
      "Training the classifier...\n",
      "Iteration 1, loss = 0.23895400\n",
      "Validation score: 0.954316\n",
      "Iteration 2, loss = 0.11658956\n",
      "Validation score: 0.965358\n",
      "Iteration 3, loss = 0.07660645\n",
      "Validation score: 0.970002\n",
      "Iteration 4, loss = 0.04521582\n",
      "Validation score: 0.971722\n",
      "Iteration 5, loss = 0.02326983\n",
      "Validation score: 0.971206\n",
      "Iteration 6, loss = 0.01264077\n",
      "Validation score: 0.972514\n",
      "Iteration 7, loss = 0.00984023\n",
      "Validation score: 0.972273\n",
      "Iteration 8, loss = 0.00843264\n",
      "Validation score: 0.971791\n",
      "Iteration 9, loss = 0.00750643\n",
      "Validation score: 0.971034\n",
      "Iteration 10, loss = 0.00728236\n",
      "Validation score: 0.969486\n",
      "Iteration 11, loss = 0.00664413\n",
      "Validation score: 0.970002\n",
      "Iteration 12, loss = 0.00621056\n",
      "Validation score: 0.966356\n",
      "Iteration 13, loss = 0.00608037\n",
      "Validation score: 0.968833\n",
      "Iteration 14, loss = 0.00632097\n",
      "Validation score: 0.969693\n",
      "Iteration 15, loss = 0.00641460\n",
      "Validation score: 0.970243\n",
      "Iteration 16, loss = 0.00526223\n",
      "Validation score: 0.971206\n",
      "Iteration 17, loss = 0.00488187\n",
      "Validation score: 0.971344\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.20694872\n",
      "Validation score: 0.969039\n",
      "Iteration 2, loss = 0.07446402\n",
      "Validation score: 0.978190\n",
      "Iteration 3, loss = 0.04717461\n",
      "Validation score: 0.982284\n",
      "Iteration 4, loss = 0.02888577\n",
      "Validation score: 0.984313\n",
      "Iteration 5, loss = 0.01548789\n",
      "Validation score: 0.985104\n",
      "Iteration 6, loss = 0.00955794\n",
      "Validation score: 0.984176\n",
      "Iteration 7, loss = 0.00753765\n",
      "Validation score: 0.984520\n",
      "Iteration 8, loss = 0.00649071\n",
      "Validation score: 0.984107\n",
      "Iteration 9, loss = 0.00583303\n",
      "Validation score: 0.984657\n",
      "Iteration 10, loss = 0.00545602\n",
      "Validation score: 0.984210\n",
      "Iteration 11, loss = 0.00533434\n",
      "Validation score: 0.984829\n",
      "Iteration 12, loss = 0.00503029\n",
      "Validation score: 0.984554\n",
      "Iteration 13, loss = 0.00438304\n",
      "Validation score: 0.984279\n",
      "Iteration 14, loss = 0.00440581\n",
      "Validation score: 0.983866\n",
      "Iteration 15, loss = 0.00442337\n",
      "Validation score: 0.983040\n",
      "Iteration 16, loss = 0.00421510\n",
      "Validation score: 0.983763\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.21050107\n",
      "Validation score: 0.963122\n",
      "Iteration 2, loss = 0.08723362\n",
      "Validation score: 0.975025\n",
      "Iteration 3, loss = 0.05229394\n",
      "Validation score: 0.978224\n",
      "Iteration 4, loss = 0.02695886\n",
      "Validation score: 0.979703\n",
      "Iteration 5, loss = 0.01284310\n",
      "Validation score: 0.980839\n",
      "Iteration 6, loss = 0.00817570\n",
      "Validation score: 0.979428\n",
      "Iteration 7, loss = 0.00690950\n",
      "Validation score: 0.978775\n",
      "Iteration 8, loss = 0.00704758\n",
      "Validation score: 0.977811\n",
      "Iteration 9, loss = 0.00634538\n",
      "Validation score: 0.978843\n",
      "Iteration 10, loss = 0.00531250\n",
      "Validation score: 0.979119\n",
      "Iteration 11, loss = 0.00469106\n",
      "Validation score: 0.979325\n",
      "Iteration 12, loss = 0.00471178\n",
      "Validation score: 0.977399\n",
      "Iteration 13, loss = 0.00561023\n",
      "Validation score: 0.977880\n",
      "Iteration 14, loss = 0.00452833\n",
      "Validation score: 0.977983\n",
      "Iteration 15, loss = 0.00420595\n",
      "Validation score: 0.975679\n",
      "Iteration 16, loss = 0.00425608\n",
      "Validation score: 0.978052\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.22186577\n",
      "Validation score: 0.958753\n",
      "Iteration 2, loss = 0.09917937\n",
      "Validation score: 0.968833\n",
      "Iteration 3, loss = 0.07108298\n",
      "Validation score: 0.975266\n",
      "Iteration 4, loss = 0.04812406\n",
      "Validation score: 0.978293\n",
      "Iteration 5, loss = 0.02751859\n",
      "Validation score: 0.979807\n",
      "Iteration 6, loss = 0.01560818\n",
      "Validation score: 0.977502\n",
      "Iteration 7, loss = 0.01103368\n",
      "Validation score: 0.979119\n",
      "Iteration 8, loss = 0.00998088\n",
      "Validation score: 0.978912\n",
      "Iteration 9, loss = 0.00881915\n",
      "Validation score: 0.978121\n",
      "Iteration 10, loss = 0.00754358\n",
      "Validation score: 0.979875\n",
      "Iteration 11, loss = 0.00698104\n",
      "Validation score: 0.979359\n",
      "Iteration 12, loss = 0.00612007\n",
      "Validation score: 0.979910\n",
      "Iteration 13, loss = 0.00536021\n",
      "Validation score: 0.978878\n",
      "Iteration 14, loss = 0.00534782\n",
      "Validation score: 0.977399\n",
      "Iteration 15, loss = 0.00533266\n",
      "Validation score: 0.977399\n",
      "Iteration 16, loss = 0.00452042\n",
      "Validation score: 0.978052\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.19902028\n",
      "Validation score: 0.973236\n",
      "Iteration 2, loss = 0.05804666\n",
      "Validation score: 0.985173\n",
      "Iteration 3, loss = 0.03740446\n",
      "Validation score: 0.986824\n",
      "Iteration 4, loss = 0.02620931\n",
      "Validation score: 0.989233\n",
      "Iteration 5, loss = 0.01820349\n",
      "Validation score: 0.991434\n",
      "Iteration 6, loss = 0.01324296\n",
      "Validation score: 0.991744\n",
      "Iteration 7, loss = 0.01016150\n",
      "Validation score: 0.991675\n",
      "Iteration 8, loss = 0.00864203\n",
      "Validation score: 0.992122\n",
      "Iteration 9, loss = 0.00783044\n",
      "Validation score: 0.991916\n",
      "Iteration 10, loss = 0.00737405\n",
      "Validation score: 0.992053\n",
      "Iteration 11, loss = 0.00627639\n",
      "Validation score: 0.991641\n",
      "Iteration 12, loss = 0.00628319\n",
      "Validation score: 0.992638\n",
      "Iteration 13, loss = 0.00514478\n",
      "Validation score: 0.992569\n",
      "Iteration 14, loss = 0.00499422\n",
      "Validation score: 0.992191\n",
      "Iteration 15, loss = 0.00469839\n",
      "Validation score: 0.992397\n",
      "Iteration 16, loss = 0.00444263\n",
      "Validation score: 0.992707\n",
      "Iteration 17, loss = 0.00434357\n",
      "Validation score: 0.991606\n",
      "Iteration 18, loss = 0.00447409\n",
      "Validation score: 0.991985\n",
      "Iteration 19, loss = 0.00384734\n",
      "Validation score: 0.991916\n",
      "Iteration 20, loss = 0.00339405\n",
      "Validation score: 0.992294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hila/Jingo-1/jupyter-env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.20385293\n",
      "Validation score: 0.970828\n",
      "Iteration 2, loss = 0.06994493\n",
      "Validation score: 0.981252\n",
      "Iteration 3, loss = 0.04654906\n",
      "Validation score: 0.985483\n",
      "Iteration 4, loss = 0.03115812\n",
      "Validation score: 0.986687\n",
      "Iteration 5, loss = 0.02021612\n",
      "Validation score: 0.987375\n",
      "Iteration 6, loss = 0.01288493\n",
      "Validation score: 0.989473\n",
      "Iteration 7, loss = 0.00958898\n",
      "Validation score: 0.989233\n",
      "Iteration 8, loss = 0.00745561\n",
      "Validation score: 0.989852\n",
      "Iteration 9, loss = 0.00721018\n",
      "Validation score: 0.989989\n",
      "Iteration 10, loss = 0.00660443\n",
      "Validation score: 0.989955\n",
      "Iteration 11, loss = 0.00531185\n",
      "Validation score: 0.990093\n",
      "Iteration 12, loss = 0.00546437\n",
      "Validation score: 0.989989\n",
      "Iteration 13, loss = 0.00485167\n",
      "Validation score: 0.989852\n",
      "Iteration 14, loss = 0.00426293\n",
      "Validation score: 0.989645\n",
      "Iteration 15, loss = 0.00441588\n",
      "Validation score: 0.988889\n",
      "Iteration 16, loss = 0.00407778\n",
      "Validation score: 0.989783\n",
      "Iteration 17, loss = 0.00351831\n",
      "Validation score: 0.989405\n",
      "Iteration 18, loss = 0.00340222\n",
      "Validation score: 0.989611\n",
      "Iteration 19, loss = 0.00356269\n",
      "Validation score: 0.989439\n",
      "Iteration 20, loss = 0.00359236\n",
      "Validation score: 0.989473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hila/Jingo-1/jupyter-env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.22539902\n",
      "Validation score: 0.967904\n",
      "Iteration 2, loss = 0.08613548\n",
      "Validation score: 0.977227\n",
      "Iteration 3, loss = 0.05887486\n",
      "Validation score: 0.982456\n",
      "Iteration 4, loss = 0.03782410\n",
      "Validation score: 0.984623\n",
      "Iteration 5, loss = 0.02183487\n",
      "Validation score: 0.983900\n",
      "Iteration 6, loss = 0.01388771\n",
      "Validation score: 0.986171\n",
      "Iteration 7, loss = 0.01025008\n",
      "Validation score: 0.985448\n",
      "Iteration 8, loss = 0.00877746\n",
      "Validation score: 0.985483\n",
      "Iteration 9, loss = 0.00841056\n",
      "Validation score: 0.985242\n",
      "Iteration 10, loss = 0.00756211\n",
      "Validation score: 0.985345\n",
      "Iteration 11, loss = 0.00622979\n",
      "Validation score: 0.984004\n",
      "Iteration 12, loss = 0.00642096\n",
      "Validation score: 0.984829\n",
      "Iteration 13, loss = 0.00566528\n",
      "Validation score: 0.984107\n",
      "Iteration 14, loss = 0.00532509\n",
      "Validation score: 0.985655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hila/Jingo-1/jupyter-env/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import hamming_loss, f1_score, classification_report\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import joblib\n",
    "from collections import Counter\n",
    "from category import categories, multi_word_keywords\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "\n",
    "def clean_lyrics(lyrics):\n",
    "    \"\"\"Clean lyrics by converting to lowercase, splitting hyphens, and removing special characters.\"\"\"\n",
    "    lyrics = str(lyrics).lower()  # Convert to string to handle non-string inputs\n",
    "    # Handle possessives (e.g., cat's → cat)\n",
    "    lyrics = re.sub(r'\\b(\\w+)\\'s\\b', r'\\1', lyrics)  # Remove 's\n",
    "    lyrics = re.sub(r'\\b(\\w+)\\'\\b', r'\\1', lyrics)  # Remove standalone '\n",
    "    # lyrics = lyrics.replace('-', ' ')  # Replace hyphens with spaces to split words\n",
    "\n",
    "    # Preserve multi-word keywords by replacing spaces with underscores\n",
    "    for phrase in multi_word_keywords:\n",
    "        lyrics = lyrics.replace(phrase, phrase.replace(' ', '_'))\n",
    "\n",
    "    lyrics = re.sub(r'[^\\w\\s]', '', lyrics)  # Remove special characters except spaces\n",
    "    return lyrics\n",
    "\n",
    "\n",
    "def assign_categories_and_words(lyrics):\n",
    "    \"\"\"Assign categories and track relevant words based on unique word occurrences.\"\"\"\n",
    "    words = set(lyrics.split())  # Use set to get unique words\n",
    "    assigned_categories = []\n",
    "    category_words = []\n",
    "    \n",
    "    for category, keywords in categories.items():\n",
    "        # Find matched keywords, converting underscores to spaces for comparison\n",
    "        matched_words = [word.replace('_', ' ') for word in words if word.replace('_', ' ') in keywords]\n",
    "        # Ensure at least three different words\n",
    "        if len(matched_words) >= 1:\n",
    "            assigned_categories.append(category)\n",
    "            category_words.append(matched_words)\n",
    "    \n",
    "    if not assigned_categories:\n",
    "        return [\"None\"], [\"None\"]\n",
    "    return assigned_categories, category_words\n",
    "\n",
    "def preprocess_dataset(input_file, output_file, lyrics_column='Lyrics'):\n",
    "    \"\"\"Read dataset, process lyrics, assign categories and words, and save to new CSV.\"\"\"\n",
    "    try:\n",
    "        # Read the input CSV\n",
    "        df = pd.read_csv(input_file)\n",
    "        print(f\"Total songs in dataset: {input_file}: {len(df)}\")\n",
    "        # Check if required columns exist\n",
    "        required_columns = [lyrics_column, 'Song', 'Artist', 'Genre']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            print(f\"Error: Missing columns {missing_columns} in {input_file}\")\n",
    "            print(\"Available columns:\", df.columns.tolist())\n",
    "            raise KeyError(f\"Missing columns: {missing_columns}\")\n",
    "        \n",
    "        # Clean lyrics\n",
    "        df['cleaned_lyrics'] = df[lyrics_column].apply(clean_lyrics)\n",
    "        \n",
    "        # Assign categories and relevant words\n",
    "        df[['categories', 'category_words']] = df['cleaned_lyrics'].apply(\n",
    "            lambda x: pd.Series(assign_categories_and_words(x))\n",
    "        )\n",
    "        \n",
    "        # Convert category_words to string representation for CSV storage\n",
    "        df['category_words'] = df['category_words'].apply(\n",
    "            lambda x: json.dumps(x) if x != [\"None\"] else \"[]\"\n",
    "        )\n",
    "        \n",
    "        # Print label distribution\n",
    "        print(\"\\nLabel distribution in processed dataset:\")\n",
    "        label_counts = df['categories'].value_counts()\n",
    "        print(label_counts)\n",
    "        print(f\"Number of songs with categories (non-['None']): {len(df[df['categories'] != '[\\'None\\']'])}\")\n",
    "        print(f\"Number of songs with no categories (['None']): {len(df[df['categories'] == '[\\'None\\']'])}\")\n",
    "        \n",
    "        # Select only the requested columns for output\n",
    "        output_columns = ['Song', 'Artist', 'Genre', 'categories', 'category_words', 'cleaned_lyrics']\n",
    "        df = df[output_columns]\n",
    "        \n",
    "        # Save the processed dataset\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Processed dataset saved to {output_file}\")\n",
    "        print(f\"Total songs in processed dataset: {len(df)}\")\n",
    "        return df\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {input_file} was not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "def print_most_wrongly_predicted_labels(test_df, y_test, y_pred, mlb, top_n=3, examples_per_category=5):\n",
    "    \"\"\"Print the most frequently mispredicted labels and example songs.\"\"\"\n",
    "    print(\"\\nAnalyzing most wrongly predicted labels...\")\n",
    "    mispredicted_labels = []\n",
    "    mispredicted_details = []\n",
    "\n",
    "    # Iterate over test samples\n",
    "    for i in range(len(test_df)):\n",
    "        true_labels = set(mlb.inverse_transform(y_test[i:i+1])[0])\n",
    "        pred_labels = set(mlb.inverse_transform(y_pred[i:i+1])[0])\n",
    "        \n",
    "        # Find incorrect labels (false positives and false negatives)\n",
    "        false_positives = pred_labels - true_labels  # Predicted but not true\n",
    "        false_negatives = true_labels - pred_labels  # True but not predicted\n",
    "        incorrect_labels = false_positives.union(false_negatives)\n",
    "        \n",
    "        if incorrect_labels:\n",
    "            song = test_df['Song'].iloc[i]\n",
    "            artist = test_df['Artist'].iloc[i]\n",
    "            genre = test_df['Genre'].iloc[i]\n",
    "            category_words = test_df['category_words'].iloc[i]\n",
    "            mispredicted_labels.extend(incorrect_labels)\n",
    "            mispredicted_details.append({\n",
    "                'song': song,\n",
    "                'artist': artist,\n",
    "                'genre': genre,\n",
    "                'true_labels': true_labels,\n",
    "                'pred_labels': pred_labels,\n",
    "                'category_words': category_words,\n",
    "                'incorrect_labels': incorrect_labels\n",
    "            })\n",
    "    \n",
    "    if not mispredicted_labels:\n",
    "        print(\"No mispredictions found in the test set.\")\n",
    "        return\n",
    "    \n",
    "    # Count frequency of mispredicted labels\n",
    "    label_counts = Counter(mispredicted_labels)\n",
    "    top_mispredicted = label_counts.most_common(top_n)\n",
    "    \n",
    "    print(f\"\\nTop {top_n} most frequently mispredicted categories:\")\n",
    "    for label, count in top_mispredicted:\n",
    "        print(f\"Category: {label}, Mispredicted {count} times\")\n",
    "        # Find example songs for this label\n",
    "        examples = [d for d in mispredicted_details if label in d['incorrect_labels']][:examples_per_category]\n",
    "        print(\"Example songs with mispredictions:\")\n",
    "        for ex in examples:\n",
    "            print(f\"  Song: {ex['song']}\")\n",
    "            print(f\"  Artist: {ex['artist']}\")\n",
    "            print(f\"  Genre: {ex['genre']}\")\n",
    "            print(f\"  True Categories: {ex['true_labels']}\")\n",
    "            print(f\"  Predicted Categories: {ex['pred_labels']}\")\n",
    "            print(f\"  Relevant Words: {ex['category_words']}\")\n",
    "            print()\n",
    "    \n",
    "    # Recommendation\n",
    "    print(\"Recommendation: Add more training songs containing keywords from these categories:\")\n",
    "    for label, _ in top_mispredicted:\n",
    "        keywords = categories[label][:5]  # Show first 5 keywords as examples\n",
    "        print(f\"  - {label}: e.g., {', '.join(keywords)}\")\n",
    "\n",
    "def train_multi_label_classifier(train_file, test_file, model_output_path, vectorizer_output_path):\n",
    "    \"\"\"Train a multi-label classifier using TF-IDF features and save the model.\"\"\"\n",
    "    # Preprocess training data\n",
    "    print(\"Preprocessing training data...\")\n",
    "    train_df = preprocess_dataset(train_file, \"../data/processed_train_dataset.csv\")\n",
    "    if train_df is None:\n",
    "        return\n",
    "    \n",
    "    # Preprocess test data (assuming already processed, but load for consistency)\n",
    "    print(\"Loading test data...\")\n",
    "    try:\n",
    "        test_df = pd.read_csv(test_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {test_file} was not found.\")\n",
    "        return\n",
    "    \n",
    "    # Prepare labels\n",
    "    mlb = MultiLabelBinarizer(classes=list(categories.keys()))\n",
    "    train_df['categories'] = train_df['categories'].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "    train_df['categories'] = train_df['categories'].apply(lambda x: [] if x == ['None'] else x)\n",
    "    y_train = mlb.fit_transform(train_df['categories'])\n",
    "    \n",
    "    test_df['categories'] = test_df['categories'].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "    test_df['categories'] = test_df['categories'].apply(lambda x: [] if x == ['None'] else x)\n",
    "    y_test = mlb.transform(test_df['categories'])\n",
    "    \n",
    "    # Extract TF-IDF features\n",
    "    vectorizer = TfidfVectorizer(max_features=10000, stop_words='english', ngram_range=(1, 2), min_df=15)\n",
    "    X_train = vectorizer.fit_transform(train_df['cleaned_lyrics'])\n",
    "    X_test = vectorizer.transform(test_df['cleaned_lyrics'])\n",
    "    \n",
    "    # Train the classifier\n",
    "    print(\"Training the classifier...\")\n",
    "    # classifier = OneVsRestClassifier(LogisticRegression(max_iter=1000, class_weight='balanced'))\n",
    "    # classifier = OneVsRestClassifier(SVC(kernel='linear', class_weight='balanced', probability=True))\n",
    "    base_classifier = MLPClassifier(\n",
    "        hidden_layer_sizes=(100,50),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        max_iter=20,\n",
    "        random_state=42,\n",
    "        verbose=True,\n",
    "        early_stopping=True,\n",
    "        n_iter_no_change=10,\n",
    "        validation_fraction=0.1\n",
    "    )\n",
    "    classifier = OneVsRestClassifier(base_classifier)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"Evaluating on test set...\")\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    hamming = hamming_loss(y_test, y_pred)\n",
    "    f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    print(f\"Hamming Loss: {hamming:.4f}\")\n",
    "    print(f\"F1 Score (Micro): {f1_micro:.4f}\")\n",
    "    print(f\"F1 Score (Macro): {f1_macro:.4f}\")\n",
    "    \n",
    "    # Print per-label performance\n",
    "    print(\"\\nPer-label classification report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=mlb.classes_, zero_division=0))\n",
    "    \n",
    "    # Print most wrongly predicted labels\n",
    "    print_most_wrongly_predicted_labels(test_df, y_test, y_pred, mlb, top_n=1, examples_per_category=5)\n",
    "\n",
    "    # Save the model and vectorizer\n",
    "    joblib.dump(classifier, model_output_path)\n",
    "    joblib.dump(vectorizer, vectorizer_output_path)\n",
    "    print(f\"Model saved to {model_output_path}\")\n",
    "    print(f\"Vectorizer saved to {vectorizer_output_path}\")\n",
    "    \n",
    "    \n",
    "    # Example predictions\n",
    "    print(\"\\nExample predictions on test set:\")\n",
    "    for i in range(min(5, len(test_df))):\n",
    "        song = test_df['Song'].iloc[i]\n",
    "        true_labels = mlb.inverse_transform(y_test[i:i+1])[0]\n",
    "        pred_labels = mlb.inverse_transform(y_pred[i:i+1])[0]\n",
    "        print(f\"Song: {song}\")\n",
    "        print(f\"True Categories: {true_labels}\")\n",
    "        print(f\"Predicted Categories: {pred_labels}\")\n",
    "        print()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    original_train = pd.read_csv(\"../data/train.csv\")\n",
    "    additional = pd.read_csv(\"../data/More_song_train.csv\")\n",
    "    # Merging the two datasets\n",
    "    merged = pd.concat([original_train, additional], ignore_index=True)\n",
    "\n",
    "    # Save the merged dataset\n",
    "    merged.to_csv(\"../data/train_with_additional.csv\",index=False)\n",
    "\n",
    "    train_file = \"../data/train_with_additional.csv\"\n",
    "    test_file = \"../data/processed_test_dataset.csv\"\n",
    "    model_output_path = \"../models/multi_label_classifier.pkl\"\n",
    "    vectorizer_output_path = \"../models/tfidf_vectorizer.pkl\"\n",
    "    train_multi_label_classifier(train_file, test_file, model_output_path, vectorizer_output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
