{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa988391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2319/3567306668.py:55: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total songs in dataset: ../data/train_with_additional.csv: 290681\n",
      "\n",
      "Label distribution in processed dataset:\n",
      "categories\n",
      "[None]                                          42545\n",
      "[Emotions]                                      22188\n",
      "[Emotions, Body Parts]                          21887\n",
      "[Emotions, Body Parts, Family]                  13130\n",
      "[Emotions, Family]                              12391\n",
      "                                                ...  \n",
      "[Animals, Clothing, Food, Sports]                  32\n",
      "[Animals, Clothing, Food, Family, Transport]       30\n",
      "[Animals, Clothing, Family, Transport]             25\n",
      "[Animals, Clothing, Food, Family, Sports]          21\n",
      "[Animals, Clothing, Food, Transport]               17\n",
      "Name: count, Length: 256, dtype: int64\n",
      "Number of songs with categories (non-['None']): 290681\n",
      "Number of songs with no categories (['None']): 0\n",
      "Processed dataset saved to ../data/processed_train_dataset.csv\n",
      "Total songs in processed dataset: 290681\n",
      "Loading test data...\n",
      "Training the classifier...\n",
      "Iteration 1, loss = 0.23803606\n",
      "Validation score: 0.952940\n",
      "Iteration 2, loss = 0.11773400\n",
      "Validation score: 0.964533\n",
      "Iteration 3, loss = 0.07690999\n",
      "Validation score: 0.971138\n",
      "Iteration 4, loss = 0.04484577\n",
      "Validation score: 0.973477\n",
      "Iteration 5, loss = 0.02298460\n",
      "Validation score: 0.974578\n",
      "Iteration 6, loss = 0.01274422\n",
      "Validation score: 0.973408\n",
      "Iteration 7, loss = 0.00897597\n",
      "Validation score: 0.972961\n",
      "Iteration 8, loss = 0.00891123\n",
      "Validation score: 0.972858\n",
      "Iteration 9, loss = 0.00825887\n",
      "Validation score: 0.971447\n",
      "Iteration 10, loss = 0.00696987\n",
      "Validation score: 0.972170\n",
      "Iteration 11, loss = 0.00670267\n",
      "Validation score: 0.971998\n",
      "Iteration 12, loss = 0.00611103\n",
      "Validation score: 0.972582\n",
      "Iteration 13, loss = 0.00527670\n",
      "Validation score: 0.973683\n",
      "Iteration 14, loss = 0.00624387\n",
      "Validation score: 0.967182\n",
      "Iteration 15, loss = 0.00665988\n",
      "Validation score: 0.972892\n",
      "Iteration 16, loss = 0.00573896\n",
      "Validation score: 0.967526\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.21354388\n",
      "Validation score: 0.968076\n",
      "Iteration 2, loss = 0.07652062\n",
      "Validation score: 0.977433\n",
      "Iteration 3, loss = 0.04641161\n",
      "Validation score: 0.980598\n",
      "Iteration 4, loss = 0.02661717\n",
      "Validation score: 0.982318\n",
      "Iteration 5, loss = 0.01422206\n",
      "Validation score: 0.982249\n",
      "Iteration 6, loss = 0.00874140\n",
      "Validation score: 0.983660\n",
      "Iteration 7, loss = 0.00735485\n",
      "Validation score: 0.983006\n",
      "Iteration 8, loss = 0.00625189\n",
      "Validation score: 0.982318\n",
      "Iteration 9, loss = 0.00582455\n",
      "Validation score: 0.983522\n",
      "Iteration 10, loss = 0.00575308\n",
      "Validation score: 0.981252\n",
      "Iteration 11, loss = 0.00607961\n",
      "Validation score: 0.981940\n",
      "Iteration 12, loss = 0.00507094\n",
      "Validation score: 0.983488\n",
      "Iteration 13, loss = 0.00402572\n",
      "Validation score: 0.982834\n",
      "Iteration 14, loss = 0.00405643\n",
      "Validation score: 0.981630\n",
      "Iteration 15, loss = 0.00465224\n",
      "Validation score: 0.982903\n",
      "Iteration 16, loss = 0.00409306\n",
      "Validation score: 0.982146\n",
      "Iteration 17, loss = 0.00392870\n",
      "Validation score: 0.983040\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.21087989\n",
      "Validation score: 0.963363\n",
      "Iteration 2, loss = 0.08783323\n",
      "Validation score: 0.975403\n",
      "Iteration 3, loss = 0.05453098\n",
      "Validation score: 0.977467\n",
      "Iteration 4, loss = 0.03099458\n",
      "Validation score: 0.978190\n",
      "Iteration 5, loss = 0.01568146\n",
      "Validation score: 0.981424\n",
      "Iteration 6, loss = 0.00926937\n",
      "Validation score: 0.980151\n",
      "Iteration 7, loss = 0.00770366\n",
      "Validation score: 0.980873\n",
      "Iteration 8, loss = 0.00645001\n",
      "Validation score: 0.980082\n",
      "Iteration 9, loss = 0.00596265\n",
      "Validation score: 0.980323\n",
      "Iteration 10, loss = 0.00515672\n",
      "Validation score: 0.978499\n",
      "Iteration 11, loss = 0.00564265\n",
      "Validation score: 0.978775\n",
      "Iteration 12, loss = 0.00490095\n",
      "Validation score: 0.979119\n",
      "Iteration 13, loss = 0.00417540\n",
      "Validation score: 0.980770\n",
      "Iteration 14, loss = 0.00431482\n",
      "Validation score: 0.978947\n",
      "Iteration 15, loss = 0.00432396\n",
      "Validation score: 0.979050\n",
      "Iteration 16, loss = 0.00413683\n",
      "Validation score: 0.977846\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.20336742\n",
      "Validation score: 0.961058\n",
      "Iteration 2, loss = 0.08760406\n",
      "Validation score: 0.973098\n",
      "Iteration 3, loss = 0.06050276\n",
      "Validation score: 0.977399\n",
      "Iteration 4, loss = 0.04075729\n",
      "Validation score: 0.981045\n",
      "Iteration 5, loss = 0.02628296\n",
      "Validation score: 0.980151\n",
      "Iteration 6, loss = 0.01677824\n",
      "Validation score: 0.981114\n",
      "Iteration 7, loss = 0.01203058\n",
      "Validation score: 0.979841\n",
      "Iteration 8, loss = 0.01022088\n",
      "Validation score: 0.979497\n",
      "Iteration 9, loss = 0.00925892\n",
      "Validation score: 0.980288\n",
      "Iteration 10, loss = 0.00808307\n",
      "Validation score: 0.979256\n",
      "Iteration 11, loss = 0.00677590\n",
      "Validation score: 0.976573\n",
      "Iteration 12, loss = 0.00639023\n",
      "Validation score: 0.979256\n",
      "Iteration 13, loss = 0.00609314\n",
      "Validation score: 0.979531\n",
      "Iteration 14, loss = 0.00521312\n",
      "Validation score: 0.979050\n",
      "Iteration 15, loss = 0.00540852\n",
      "Validation score: 0.978052\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.19708830\n",
      "Validation score: 0.971138\n",
      "Iteration 2, loss = 0.05576538\n",
      "Validation score: 0.984348\n",
      "Iteration 3, loss = 0.03397980\n",
      "Validation score: 0.986618\n",
      "Iteration 4, loss = 0.02394528\n",
      "Validation score: 0.990058\n",
      "Iteration 5, loss = 0.01616228\n",
      "Validation score: 0.990643\n",
      "Iteration 6, loss = 0.01131357\n",
      "Validation score: 0.990402\n",
      "Iteration 7, loss = 0.00959055\n",
      "Validation score: 0.991985\n",
      "Iteration 8, loss = 0.00834804\n",
      "Validation score: 0.991606\n",
      "Iteration 9, loss = 0.00704160\n",
      "Validation score: 0.991434\n",
      "Iteration 10, loss = 0.00683347\n",
      "Validation score: 0.991813\n",
      "Iteration 11, loss = 0.00552216\n",
      "Validation score: 0.991881\n",
      "Iteration 12, loss = 0.00519410\n",
      "Validation score: 0.990918\n",
      "Iteration 13, loss = 0.00470632\n",
      "Validation score: 0.991400\n",
      "Iteration 14, loss = 0.00483069\n",
      "Validation score: 0.991090\n",
      "Iteration 15, loss = 0.00440756\n",
      "Validation score: 0.991021\n",
      "Iteration 16, loss = 0.00397711\n",
      "Validation score: 0.991606\n",
      "Iteration 17, loss = 0.00375010\n",
      "Validation score: 0.991778\n",
      "Iteration 18, loss = 0.00397293\n",
      "Validation score: 0.991847\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.20355361\n",
      "Validation score: 0.968145\n",
      "Iteration 2, loss = 0.07250590\n",
      "Validation score: 0.981596\n",
      "Iteration 3, loss = 0.04897251\n",
      "Validation score: 0.985242\n",
      "Iteration 4, loss = 0.03217994\n",
      "Validation score: 0.987960\n",
      "Iteration 5, loss = 0.02027191\n",
      "Validation score: 0.987581\n",
      "Iteration 6, loss = 0.01280727\n",
      "Validation score: 0.987856\n",
      "Iteration 7, loss = 0.00910511\n",
      "Validation score: 0.988648\n",
      "Iteration 8, loss = 0.00783883\n",
      "Validation score: 0.987925\n",
      "Iteration 9, loss = 0.00579939\n",
      "Validation score: 0.989577\n",
      "Iteration 10, loss = 0.00620888\n",
      "Validation score: 0.988613\n",
      "Iteration 11, loss = 0.00555947\n",
      "Validation score: 0.988510\n",
      "Iteration 12, loss = 0.00474983\n",
      "Validation score: 0.989026\n",
      "Iteration 13, loss = 0.00456919\n",
      "Validation score: 0.988751\n",
      "Iteration 14, loss = 0.00412204\n",
      "Validation score: 0.988923\n",
      "Iteration 15, loss = 0.00373935\n",
      "Validation score: 0.988579\n",
      "Iteration 16, loss = 0.00398304\n",
      "Validation score: 0.988992\n",
      "Iteration 17, loss = 0.00353844\n",
      "Validation score: 0.989817\n",
      "Iteration 18, loss = 0.00287484\n",
      "Validation score: 0.989026\n",
      "Iteration 19, loss = 0.00323708\n",
      "Validation score: 0.988579\n",
      "Iteration 20, loss = 0.00311228\n",
      "Validation score: 0.988957\n",
      "Iteration 21, loss = 0.00349425\n",
      "Validation score: 0.987925\n",
      "Iteration 22, loss = 0.00303048\n",
      "Validation score: 0.989164\n",
      "Iteration 23, loss = 0.00272773\n",
      "Validation score: 0.988957\n",
      "Iteration 24, loss = 0.00284868\n",
      "Validation score: 0.988510\n",
      "Iteration 25, loss = 0.00314858\n",
      "Validation score: 0.989095\n",
      "Iteration 26, loss = 0.00324786\n",
      "Validation score: 0.988854\n",
      "Iteration 27, loss = 0.00280959\n",
      "Validation score: 0.989473\n",
      "Iteration 28, loss = 0.00268704\n",
      "Validation score: 0.988407\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.22700153\n",
      "Validation score: 0.963260\n",
      "Iteration 2, loss = 0.09062727\n",
      "Validation score: 0.977055\n",
      "Iteration 3, loss = 0.06144777\n",
      "Validation score: 0.980323\n",
      "Iteration 4, loss = 0.04040973\n",
      "Validation score: 0.981355\n",
      "Iteration 5, loss = 0.02385067\n",
      "Validation score: 0.983969\n",
      "Iteration 6, loss = 0.01385339\n",
      "Validation score: 0.982868\n",
      "Iteration 7, loss = 0.00996877\n",
      "Validation score: 0.982559\n",
      "Iteration 8, loss = 0.00870092\n",
      "Validation score: 0.983109\n",
      "Iteration 9, loss = 0.00856814\n",
      "Validation score: 0.982284\n",
      "Iteration 10, loss = 0.00689310\n",
      "Validation score: 0.981699\n",
      "Iteration 11, loss = 0.00594054\n",
      "Validation score: 0.982421\n",
      "Iteration 12, loss = 0.00564380\n",
      "Validation score: 0.982421\n",
      "Iteration 13, loss = 0.00555714\n",
      "Validation score: 0.982524\n",
      "Iteration 14, loss = 0.00562498\n",
      "Validation score: 0.980839\n",
      "Iteration 15, loss = 0.00524854\n",
      "Validation score: 0.982800\n",
      "Iteration 16, loss = 0.00452848\n",
      "Validation score: 0.982318\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.22826740\n",
      "Validation score: 0.962778\n",
      "Iteration 2, loss = 0.09064828\n",
      "Validation score: 0.972170\n",
      "Iteration 3, loss = 0.06116154\n",
      "Validation score: 0.978775\n",
      "Iteration 4, loss = 0.03762990\n",
      "Validation score: 0.980873\n",
      "Iteration 5, loss = 0.01979527\n",
      "Validation score: 0.981355\n",
      "Iteration 6, loss = 0.01331820\n",
      "Validation score: 0.983109\n",
      "Iteration 7, loss = 0.00992771\n",
      "Validation score: 0.982593\n",
      "Iteration 8, loss = 0.00885930\n",
      "Validation score: 0.981492\n",
      "Iteration 9, loss = 0.00858759\n",
      "Validation score: 0.982456\n",
      "Iteration 10, loss = 0.00727495\n",
      "Validation score: 0.982937\n",
      "Iteration 11, loss = 0.00640452\n",
      "Validation score: 0.981664\n",
      "Iteration 12, loss = 0.00563207\n",
      "Validation score: 0.981458\n",
      "Iteration 13, loss = 0.00627206\n",
      "Validation score: 0.981699\n",
      "Iteration 14, loss = 0.00562562\n",
      "Validation score: 0.980667\n",
      "Iteration 15, loss = 0.00479893\n",
      "Validation score: 0.982249\n",
      "Iteration 16, loss = 0.00454877\n",
      "Validation score: 0.982249\n",
      "Iteration 17, loss = 0.00425074\n",
      "Validation score: 0.981011\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Evaluating on test set...\n",
      "Hamming Loss: 0.0244\n",
      "F1 Score (Micro): 0.9660\n",
      "F1 Score (Macro): 0.9535\n",
      "\n",
      "Per-label classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Animals       0.95      0.85      0.90      1921\n",
      "    Clothing       0.97      0.91      0.94      1695\n",
      "        Food       0.93      0.89      0.91      1508\n",
      "    Emotions       0.99      0.98      0.98      6078\n",
      "  Body Parts       1.00      0.98      0.99      4716\n",
      "      Family       0.99      0.97      0.98      3560\n",
      "   Transport       0.98      0.94      0.96      2401\n",
      "      Sports       0.98      0.95      0.96      2827\n",
      "\n",
      "   micro avg       0.98      0.95      0.97     24706\n",
      "   macro avg       0.97      0.93      0.95     24706\n",
      "weighted avg       0.98      0.95      0.97     24706\n",
      " samples avg       0.94      0.92      0.93     24706\n",
      "\n",
      "\n",
      "Analyzing most wrongly predicted labels...\n",
      "\n",
      "Top 1 most frequently mispredicted categories:\n",
      "Category: Animals, Mispredicted 375 times\n",
      "Example songs with mispredictions:\n",
      "  Song: i-tried-so-hard-remix\n",
      "  Artist: eminem\n",
      "  Genre: Hip-Hop\n",
      "  True Categories: {'Transport', 'Emotions', 'Body Parts', 'Food', 'Family', 'Sports', 'Animals'}\n",
      "  Predicted Categories: {'Transport', 'Emotions', 'Body Parts', 'Food', 'Family', 'Sports'}\n",
      "  Relevant Words: [[\"dove\"], [\"beef\", \"noodle\", \"water\"], [\"love\", \"lost\", \"exhausted\", \"boring\", \"hungry\", \"lonely\"], [\"shoulders\", \"toe\", \"mouth\", \"bone\", \"teeth\", \"ears\"], [\"partner\", \"home\", \"father\", \"daughter\"], [\"streets\", \"roads\"], [\"ball\", \"bowling\", \"crew\", \"weight\"]]\n",
      "\n",
      "  Song: you-don-t-know-me\n",
      "  Artist: black-rob\n",
      "  Genre: Hip-Hop\n",
      "  True Categories: {'Transport', 'Emotions', 'Clothing', 'Food', 'Family', 'Sports', 'Animals'}\n",
      "  Predicted Categories: {'Transport', 'Emotions', 'Clothing', 'Food', 'Family', 'Sports'}\n",
      "  Relevant Words: [[\"goat\"], [\"shoes\", \"gloves\"], [\"cake\", \"beef\", \"ice\"], [\"love\"], [\"son\"], [\"lane\"], [\"games\", \"kick\"]]\n",
      "\n",
      "  Song: love-you-no-matter-what\n",
      "  Artist: anthony-green\n",
      "  Genre: Indie\n",
      "  True Categories: {'Body Parts', 'Animals', 'Emotions'}\n",
      "  Predicted Categories: {'Body Parts', 'Emotions'}\n",
      "  Relevant Words: [[\"spiders\"], [\"insane\", \"hate\", \"love\", \"shocked\"], [\"head\"]]\n",
      "\n",
      "  Song: army-of-the-universe\n",
      "  Artist: celesty\n",
      "  Genre: Metal\n",
      "  True Categories: {'Body Parts', 'Animals', 'Sports', 'Emotions'}\n",
      "  Predicted Categories: {'Body Parts', 'Emotions', 'Sports'}\n",
      "  Relevant Words: [[\"horse\"], [\"rage\", \"lost\"], [\"eye\", \"arms\", \"eyes\", \"hand\", \"head\"], [\"horse\", \"win\"]]\n",
      "\n",
      "  Song: king-and-cross\n",
      "  Artist: asgeir\n",
      "  Genre: Indie\n",
      "  True Categories: {'Animals', 'Family', 'Sports'}\n",
      "  Predicted Categories: {'Family', 'Sports'}\n",
      "  Relevant Words: [[\"foxes\"], [\"house\", \"home\", \"child\"], [\"walking\"]]\n",
      "\n",
      "Recommendation: Add more training songs containing keywords from these categories:\n",
      "  - Animals: e.g., animal, animals, alligator, alligators, alpaca\n",
      "Model saved to ../models/multi_label_classifier.pkl\n",
      "Vectorizer saved to ../models/tfidf_vectorizer.pkl\n",
      "\n",
      "Example predictions on test set:\n",
      "Song: craftsmanship\n",
      "True Categories: ('Clothing', 'Food', 'Body Parts', 'Transport')\n",
      "Predicted Categories: ('Clothing', 'Food', 'Body Parts', 'Transport')\n",
      "\n",
      "Song: come-on-out\n",
      "True Categories: ('Animals', 'Emotions', 'Body Parts')\n",
      "Predicted Categories: ('Animals', 'Emotions', 'Body Parts')\n",
      "\n",
      "Song: riot\n",
      "True Categories: ('Emotions',)\n",
      "Predicted Categories: ('Emotions',)\n",
      "\n",
      "Song: that-s-what-girls-do\n",
      "True Categories: ('Clothing', 'Emotions', 'Body Parts')\n",
      "Predicted Categories: ('Clothing', 'Emotions', 'Body Parts')\n",
      "\n",
      "Song: believe-in-a-dollar\n",
      "True Categories: ('Body Parts', 'Family')\n",
      "Predicted Categories: ('Body Parts', 'Family')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import hamming_loss, f1_score, classification_report\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import joblib\n",
    "from collections import Counter\n",
    "from category import categories, multi_word_keywords\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import os\n",
    "folder_name = \"../models/\"  # Define the folder name for saving models\n",
    "\n",
    "\n",
    "\n",
    "def clean_lyrics(lyrics):\n",
    "    \"\"\"Clean lyrics by converting to lowercase, splitting hyphens, and removing special characters.\"\"\"\n",
    "    lyrics = str(lyrics).lower()  # Convert to string to handle non-string inputs\n",
    "    # Handle possessives (e.g., cat's â†’ cat)\n",
    "    lyrics = re.sub(r'\\b(\\w+)\\'s\\b', r'\\1', lyrics)  # Remove 's\n",
    "    lyrics = re.sub(r'\\b(\\w+)\\'\\b', r'\\1', lyrics)  # Remove standalone '\n",
    "    # lyrics = lyrics.replace('-', ' ')  # Replace hyphens with spaces to split words\n",
    "\n",
    "    # Preserve multi-word keywords by replacing spaces with underscores\n",
    "    for phrase in multi_word_keywords:\n",
    "        lyrics = lyrics.replace(phrase, phrase.replace(' ', '_'))\n",
    "        \n",
    "    lyrics = re.sub(r'[^\\w\\s]', '', lyrics)  # Remove special characters except spaces\n",
    "    return lyrics\n",
    "\n",
    "\n",
    "def assign_categories_and_words(lyrics):\n",
    "    \"\"\"Assign categories and track relevant words based on unique word occurrences.\"\"\"\n",
    "    words = set(lyrics.split())  # Use set to get unique words\n",
    "    assigned_categories = []\n",
    "    category_words = []\n",
    "    \n",
    "    for category, keywords in categories.items():\n",
    "        # Find matched keywords, converting underscores to spaces for comparison\n",
    "        matched_words = list(words & set(keywords))\n",
    "        # Ensure at least three different words\n",
    "        if len(matched_words) >= 1:\n",
    "            assigned_categories.append(category)\n",
    "            category_words.append(matched_words)\n",
    "    \n",
    "    if not assigned_categories:\n",
    "        return [\"None\"], [\"None\"]\n",
    "    return assigned_categories, category_words\n",
    "\n",
    "def preprocess_dataset(input_file, output_file, lyrics_column='Lyrics'):\n",
    "    \"\"\"Read dataset, process lyrics, assign categories and words, and save to new CSV.\"\"\"\n",
    "    try:\n",
    "        # Read the input CSV\n",
    "        df = pd.read_csv(input_file)\n",
    "        print(f\"Total songs in dataset: {input_file}: {len(df)}\")\n",
    "        # Check if required columns exist\n",
    "        required_columns = [lyrics_column, 'Song', 'Artist', 'Genre']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            print(f\"Error: Missing columns {missing_columns} in {input_file}\")\n",
    "            print(\"Available columns:\", df.columns.tolist())\n",
    "            raise KeyError(f\"Missing columns: {missing_columns}\")\n",
    "        \n",
    "        # Clean lyrics\n",
    "        df['cleaned_lyrics'] = df[lyrics_column].apply(clean_lyrics)\n",
    "        \n",
    "        # Assign categories and relevant words\n",
    "        df[['categories', 'category_words']] = df['cleaned_lyrics'].apply(\n",
    "            lambda x: pd.Series(assign_categories_and_words(x))\n",
    "        )\n",
    "        \n",
    "        # Convert category_words to string representation for CSV storage\n",
    "        df['category_words'] = df['category_words'].apply(\n",
    "            lambda x: json.dumps(x) if x != [\"None\"] else \"[]\"\n",
    "        )\n",
    "        \n",
    "        # Print label distribution\n",
    "        print(\"\\nLabel distribution in processed dataset:\")\n",
    "        label_counts = df['categories'].value_counts()\n",
    "        print(label_counts)\n",
    "        print(f\"Number of songs with categories (non-['None']): {len(df[df['categories'] != '[\\'None\\']'])}\")\n",
    "        print(f\"Number of songs with no categories (['None']): {len(df[df['categories'] == '[\\'None\\']'])}\")\n",
    "        \n",
    "        # Select only the requested columns for output\n",
    "        output_columns = ['Song', 'Artist', 'Genre', 'categories', 'category_words', 'cleaned_lyrics']\n",
    "        df = df[output_columns]\n",
    "        \n",
    "        # Save the processed dataset\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Processed dataset saved to {output_file}\")\n",
    "        print(f\"Total songs in processed dataset: {len(df)}\")\n",
    "        return df\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {input_file} was not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "def print_most_wrongly_predicted_labels(test_df, y_test, y_pred, mlb, top_n=3, examples_per_category=5):\n",
    "    \"\"\"Print the most frequently mispredicted labels and example songs.\"\"\"\n",
    "    print(\"\\nAnalyzing most wrongly predicted labels...\")\n",
    "    mispredicted_labels = []\n",
    "    mispredicted_details = []\n",
    "\n",
    "    # Iterate over test samples\n",
    "    for i in range(len(test_df)):\n",
    "        true_labels = set(mlb.inverse_transform(y_test[i:i+1])[0])\n",
    "        pred_labels = set(mlb.inverse_transform(y_pred[i:i+1])[0])\n",
    "        \n",
    "        # Find incorrect labels (false positives and false negatives)\n",
    "        false_positives = pred_labels - true_labels  # Predicted but not true\n",
    "        false_negatives = true_labels - pred_labels  # True but not predicted\n",
    "        incorrect_labels = false_positives.union(false_negatives)\n",
    "        \n",
    "        if incorrect_labels:\n",
    "            song = test_df['Song'].iloc[i]\n",
    "            artist = test_df['Artist'].iloc[i]\n",
    "            genre = test_df['Genre'].iloc[i]\n",
    "            category_words = test_df['category_words'].iloc[i]\n",
    "            mispredicted_labels.extend(incorrect_labels)\n",
    "            mispredicted_details.append({\n",
    "                'song': song,\n",
    "                'artist': artist,\n",
    "                'genre': genre,\n",
    "                'true_labels': true_labels,\n",
    "                'pred_labels': pred_labels,\n",
    "                'category_words': category_words,\n",
    "                'incorrect_labels': incorrect_labels\n",
    "            })\n",
    "    \n",
    "    if not mispredicted_labels:\n",
    "        print(\"No mispredictions found in the test set.\")\n",
    "        return\n",
    "    \n",
    "    # Count frequency of mispredicted labels\n",
    "    label_counts = Counter(mispredicted_labels)\n",
    "    top_mispredicted = label_counts.most_common(top_n)\n",
    "    \n",
    "    print(f\"\\nTop {top_n} most frequently mispredicted categories:\")\n",
    "    for label, count in top_mispredicted:\n",
    "        print(f\"Category: {label}, Mispredicted {count} times\")\n",
    "        # Find example songs for this label\n",
    "        examples = [d for d in mispredicted_details if label in d['incorrect_labels']][:examples_per_category]\n",
    "        print(\"Example songs with mispredictions:\")\n",
    "        for ex in examples:\n",
    "            print(f\"  Song: {ex['song']}\")\n",
    "            print(f\"  Artist: {ex['artist']}\")\n",
    "            print(f\"  Genre: {ex['genre']}\")\n",
    "            print(f\"  True Categories: {ex['true_labels']}\")\n",
    "            print(f\"  Predicted Categories: {ex['pred_labels']}\")\n",
    "            print(f\"  Relevant Words: {ex['category_words']}\")\n",
    "            print()\n",
    "    \n",
    "    # Recommendation\n",
    "    print(\"Recommendation: Add more training songs containing keywords from these categories:\")\n",
    "    for label, _ in top_mispredicted:\n",
    "        keywords = categories[label][:5]  # Show first 5 keywords as examples\n",
    "        print(f\"  - {label}: e.g., {', '.join(keywords)}\")\n",
    "\n",
    "def train_multi_label_classifier(train_file, test_file, model_output_path, vectorizer_output_path):\n",
    "    \"\"\"Train a multi-label classifier using TF-IDF features and save the model.\"\"\"\n",
    "    # Preprocess training data\n",
    "    print(\"Preprocessing training data...\")\n",
    "    train_df = preprocess_dataset(train_file, \"../data/processed_train_dataset.csv\")\n",
    "    if train_df is None:\n",
    "        return\n",
    "    \n",
    "    # Preprocess test data (assuming already processed, but load for consistency)\n",
    "    print(\"Loading test data...\")\n",
    "    try:\n",
    "        test_df = pd.read_csv(test_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {test_file} was not found.\")\n",
    "        return\n",
    "    \n",
    "    # Prepare labels\n",
    "    mlb = MultiLabelBinarizer(classes=list(categories.keys()))\n",
    "    train_df['categories'] = train_df['categories'].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "    train_df['categories'] = train_df['categories'].apply(lambda x: [] if x == ['None'] else x)\n",
    "    y_train = mlb.fit_transform(train_df['categories'])\n",
    "    \n",
    "    test_df['categories'] = test_df['categories'].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "    test_df['categories'] = test_df['categories'].apply(lambda x: [] if x == ['None'] else x)\n",
    "    y_test = mlb.transform(test_df['categories'])\n",
    "    \n",
    "    # Extract TF-IDF features\n",
    "    vectorizer = TfidfVectorizer(max_features=10000, stop_words='english', ngram_range=(1, 2), min_df=15)\n",
    "    X_train = vectorizer.fit_transform(train_df['cleaned_lyrics'])\n",
    "    X_test = vectorizer.transform(test_df['cleaned_lyrics'])\n",
    "    \n",
    "    # Train the classifier\n",
    "    print(\"Training the classifier...\")\n",
    "    # classifier = OneVsRestClassifier(LogisticRegression(max_iter=1000, class_weight='balanced'))\n",
    "    # classifier = OneVsRestClassifier(SVC(kernel='linear', class_weight='balanced', probability=True))\n",
    "    base_classifier = MLPClassifier(\n",
    "        hidden_layer_sizes=(100,50),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        max_iter=200,\n",
    "        random_state=42,\n",
    "        verbose=True,\n",
    "        early_stopping=True,\n",
    "        n_iter_no_change=10,\n",
    "        validation_fraction=0.1\n",
    "    )\n",
    "    classifier = OneVsRestClassifier(base_classifier)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"Evaluating on test set...\")\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    hamming = hamming_loss(y_test, y_pred)\n",
    "    f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    print(f\"Hamming Loss: {hamming:.4f}\")\n",
    "    print(f\"F1 Score (Micro): {f1_micro:.4f}\")\n",
    "    print(f\"F1 Score (Macro): {f1_macro:.4f}\")\n",
    "    \n",
    "    # Print per-label performance\n",
    "    print(\"\\nPer-label classification report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=mlb.classes_, zero_division=0))\n",
    "    \n",
    "    # Print most wrongly predicted labels\n",
    "    print_most_wrongly_predicted_labels(test_df, y_test, y_pred, mlb, top_n=1, examples_per_category=5)\n",
    "\n",
    "    # Save the model and vectorizer\n",
    "    joblib.dump(classifier, model_output_path)\n",
    "    joblib.dump(vectorizer, vectorizer_output_path)\n",
    "    print(f\"Model saved to {model_output_path}\")\n",
    "    print(f\"Vectorizer saved to {vectorizer_output_path}\")\n",
    "    \n",
    "    \n",
    "    # Example predictions\n",
    "    print(\"\\nExample predictions on test set:\")\n",
    "    for i in range(min(5, len(test_df))):\n",
    "        song = test_df['Song'].iloc[i]\n",
    "        true_labels = mlb.inverse_transform(y_test[i:i+1])[0]\n",
    "        pred_labels = mlb.inverse_transform(y_pred[i:i+1])[0]\n",
    "        print(f\"Song: {song}\")\n",
    "        print(f\"True Categories: {true_labels}\")\n",
    "        print(f\"Predicted Categories: {pred_labels}\")\n",
    "        print()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    original_train = pd.read_csv(\"../data/train.csv\")\n",
    "    additional = pd.read_csv(\"../data/More_song_train.csv\")\n",
    "    # Merging the two datasets\n",
    "    merged = pd.concat([original_train, additional], ignore_index=True)\n",
    "\n",
    "    # Save the merged dataset\n",
    "    merged.to_csv(\"../data/train_with_additional.csv\",index=False)\n",
    "\n",
    "    # Check if the folder exists\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    train_file = \"../data/train_with_additional.csv\"\n",
    "    test_file = \"../data/processed_test_dataset.csv\"\n",
    "    model_output_path = \"../models/multi_label_classifier.pkl\"\n",
    "    vectorizer_output_path = \"../models/tfidf_vectorizer.pkl\"\n",
    "    train_multi_label_classifier(train_file, test_file, model_output_path, vectorizer_output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
