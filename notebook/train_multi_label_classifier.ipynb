{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa988391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing training data...\n",
      "Processed dataset saved to ../data/processed_train_dataset.csv\n",
      "Loading test data...\n",
      "Training the classifier...\n",
      "Evaluating on test set...\n",
      "Hamming Loss: 0.0337\n",
      "F1 Score (Micro): 0.4713\n",
      "F1 Score (Macro): 0.2771\n",
      "Model saved to ../models/multi_label_classifier.pkl\n",
      "Vectorizer saved to ../models/tfidf_vectorizer.pkl\n",
      "\n",
      "Example predictions on test set:\n",
      "Song: craftsmanship\n",
      "True Categories: ('Clothing',)\n",
      "Predicted Categories: ('Clothing',)\n",
      "\n",
      "Song: come-on-out\n",
      "True Categories: ()\n",
      "Predicted Categories: ()\n",
      "\n",
      "Song: riot\n",
      "True Categories: ()\n",
      "Predicted Categories: ()\n",
      "\n",
      "Song: that-s-what-girls-do\n",
      "True Categories: ()\n",
      "Predicted Categories: ()\n",
      "\n",
      "Song: believe-in-a-dollar\n",
      "True Categories: ()\n",
      "Predicted Categories: ()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import hamming_loss, f1_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import joblib\n",
    "\n",
    "# Define categories and associated words\n",
    "categories = {\n",
    "    \"Animals\": [\n",
    "        \"alligator\", \"alligators\", \"ant\", \"ants\", \"antelope\", \"antelopes\", \"bat\", \"bats\", \"bear\", \"bears\",\n",
    "        \"bee\", \"bees\", \"beetle\", \"beetles\", \"buffalo\", \"buffalos\", \"butterfly\", \"butterflies\", \"camel\", \"camels\",\n",
    "        \"cat\", \"cats\", \"caterpillar\", \"caterpillars\", \"cheetah\", \"cheetahs\", \"chicken\", \"chickens\", \"chimpanzee\", \"chimpanzees\",\n",
    "        \"cow\", \"cows\", \"crab\", \"crabs\", \"crocodile\", \"crocodiles\", \"deer\", \"dog\", \"dogs\", \"dolphin\", \"dolphins\",\n",
    "        \"donkey\", \"donkeys\", \"duck\", \"ducks\", \"eagle\", \"eagles\", \"elephant\", \"elephants\", \"falcon\", \"falcons\", \"ferret\", \"ferrets\",\n",
    "        \"fish\", \"flamingo\", \"flamingos\", \"fox\", \"foxes\", \"frog\", \"frogs\", \"giraffe\", \"giraffes\", \"goat\", \"goats\",\n",
    "        \"goose\", \"geese\", \"gorilla\", \"gorillas\", \"grasshopper\", \"grasshoppers\", \"hamster\", \"hamsters\", \"hawk\", \"hawks\", \"hedgehog\", \"hedgehogs\",\n",
    "        \"hippo\", \"hippos\", \"horse\", \"horses\", \"hyena\", \"hyenas\", \"jaguar\", \"jaguars\", \"jellyfish\", \"kangaroo\", \"kangaroos\", \"koala\", \"koalas\",\n",
    "        \"leopard\", \"leopards\", \"lion\", \"lions\", \"lizard\", \"lizards\", \"lobster\", \"lobsters\", \"llama\", \"llamas\", \"mole\", \"moles\", \"monkey\", \"monkeys\",\n",
    "        \"moose\", \"mosquito\", \"mosquitoes\", \"mouse\", \"mice\", \"octopus\", \"octopuses\", \"ostrich\", \"ostriches\", \"otter\", \"otters\", \"owl\", \"owls\",\n",
    "        \"ox\", \"oxen\", \"panda\", \"pandas\", \"parrot\", \"parrots\", \"peacock\", \"peacocks\", \"pelican\", \"pelicans\", \"penguin\", \"penguins\", \"pig\", \"pigs\",\n",
    "        \"pigeon\", \"pigeons\", \"platypus\", \"platypuses\", \"porcupine\", \"porcupines\", \"rabbit\", \"rabbits\", \"raccoon\", \"raccoons\", \"rat\", \"rats\", \"raven\", \"ravens\",\n",
    "        \"reindeer\", \"rhino\", \"rhinos\", \"seal\", \"seals\", \"seahorse\", \"seahorses\", \"shark\", \"sharks\", \"sheep\", \"skunk\", \"skunks\", \"sloth\", \"sloths\",\n",
    "        \"snail\", \"snails\", \"snake\", \"snakes\", \"sparrow\", \"sparrows\", \"spider\", \"spiders\", \"squid\", \"squids\", \"squirrel\", \"squirrels\", \"starfish\", \"stingray\", \"stingrays\",\n",
    "        \"swan\", \"swans\", \"termite\", \"termites\", \"tiger\", \"tigers\", \"toad\", \"toads\", \"toucan\", \"toucans\", \"turtle\", \"turtles\", \"walrus\", \"walruses\",\n",
    "        \"wasp\", \"wasps\", \"whale\", \"whales\", \"wolf\", \"wolves\", \"zebra\", \"zebras\"\n",
    "    ],\n",
    "\n",
    "    \"Clothing\": [\n",
    "        \"backpack\", \"backpacks\", \"belt\", \"belts\", \"beanie\", \"beanies\", \"blazer\", \"blazers\", \"blouse\", \"blouses\",\n",
    "        \"boots\", \"bra\", \"bras\", \"briefs\", \"cape\", \"capes\", \"cardigan\", \"cardigans\", \"coat\", \"coats\",\n",
    "        \"corset\", \"corsets\", \"cufflinks\", \"dress\", \"dresses\", \"earrings\", \"espadrilles\", \"flip-flops\",\n",
    "        \"gown\", \"gowns\", \"glasses\", \"glove\", \"gloves\", \"hat\", \"hats\", \"handbag\", \"handbags\", \"heels\", \"helmet\", \"helmets\", \"hoodie\", \"hoodies\",\n",
    "        \"jacket\", \"jackets\", \"jeans\", \"jumpsuit\", \"jumpsuits\", \"kimono\", \"kimonos\", \"leggings\", \"loafers\", \"mask\", \"masks\", \"mittens\", \"necklace\", \"necklaces\",\n",
    "        \"overalls\", \"overcoat\", \"overcoats\", \"pants\", \"pajamas\", \"panties\", \"poncho\", \"ponchos\", \"purse\", \"purses\", \"raincoat\", \"raincoats\",\n",
    "        \"ring\", \"rings\", \"robe\", \"robes\", \"sandal\", \"sandals\", \"satchel\", \"satchels\", \"scarf\", \"scarves\", \"shirt\", \"shirts\",\n",
    "        \"shoe\", \"shoes\", \"shorts\", \"skirt\", \"skirts\", \"slipper\", \"slippers\", \"sneaker\", \"sneakers\", \"socks\", \"suit\", \"suits\", \"sunglasses\", \"sweater\", \"sweaters\",\n",
    "        \"t-shirt\", \"t-shirts\", \"tank_top\", \"tank_tops\", \"thobe\", \"tie\", \"ties\", \"tights\", \"tracksuit\", \"tracksuits\", \"tunic\", \"tunics\",\n",
    "        \"turban\", \"turbans\", \"tuxedo\", \"tuxedos\", \"uniform\", \"uniforms\", \"veil\", \"veils\", \"vest\", \"vests\", \"wallet\", \"wallets\", \"watch\", \"watches\", \"windbreaker\", \"windbreakers\",\n",
    "        \"wristband\", \"wristbands\"\n",
    "    ],\n",
    "\n",
    "    \"Food\": [\n",
    "        \"almond\", \"almonds\", \"apple\", \"apples\", \"avocado\", \"avocados\", \"artichoke\", \"asparagus\", \"bacon\", \"banana\", \"bananas\", \"bagel\", \"baguette\", \"barley\", \"basil\", \"beans\", \"beef\", \"blackberry\", \"blackberries\", \"blueberry\", \"blueberries\", \"bread\", \"broccoli\", \"burger\", \"burgers\",\n",
    "        \"butter\", \"cabbage\", \"cake\", \"cakes\", \"candy\", \"candies\", \"carrot\", \"carrots\", \"caramel\", \"cashew\", \"cashews\", \"cauliflower\", \"cereal\", \"cherry\", \"cherries\", \"cheese\", \"chips\", \"chickpea\",\n",
    "        \"chocolate\", \"cinnamon\", \"coconut\", \"coconuts\", \"coffee\", \"cookie\", \"cookies\", \"corn\", \"cracker\", \"crackers\", \"crepe\", \"croissant\", \"cucumber\", \"cucumbers\", \"donut\", \"donuts\",\n",
    "        \"egg\", \"eggs\", \"eggplant\", \"eggplants\", \"fish\", \"fries\", \"garlic\", \"gelato\", \"ginger\", \"grape\", \"grapes\", \"grapefruit\", \"ham\", \"hamburger\", \"hamburgers\", \"hazelnut\", \"hazelnuts\", \"honey\", \"hotdog\", \"hotdogs\",\n",
    "        \"ice_cream\", \"jam\", \"jelly\", \"juice\", \"ketchup\", \"kebab\", \"kiwi\", \"kiwis\", \"lasagna\", \"lentil\", \"licorice\", \"lemon\", \"lemons\", \"lettuce\", \"lime\", \"limes\", \"lobster\", \"lobsters\", \"macadamia\", \"macaroni\", \"macaron\", \"mayonnaise\", \"meatball\", \"mint\", \"mango\", \"mangos\",\n",
    "        \"meat\", \"melon\", \"melons\", \"milk\", \"mousse\", \"muffin\", \"muffins\", \"mushroom\", \"mushrooms\", \"mustard\", \"noodle\", \"noodles\", \"nut\", \"nuts\", \"oat\", \"oats\", \"okra\", \"oregano\", \"oyster\", \"oatmeal\", \"olive\", \"olives\",\n",
    "        \"onion\", \"onions\", \"orange\", \"oranges\", \"pancake\", \"pancakes\", \"pasta\", \"pastry\", \"peach\", \"peaches\", \"peanut\", \"peanuts\", \"pear\", \"pears\", \"peas\", \"pepper\", \"peppers\", \"pecan\", \"pesto\", \"pickle\", \"pistachio\",\n",
    "        \"pie\", \"pies\", \"pineapple\", \"pineapples\", \"pizza\", \"plum\", \"plums\", \"pork\", \"popcorn\", \"potato\", \"potatoes\", \"pumpkin\" \"pretzel\", \"pretzels\", \"raisin\", \"raisins\", \"raspberry\", \"raspberries\",\n",
    "        \"rice\", \"salad\", \"salmon\", \"sandwich\", \"sandwiches\", \"sausage\", \"sausages\", \"shrimp\", \"smoothie\", \"snack\", \"snacks\", \"soda\", \"soup\", \"spinach\", \"steak\", \"strawberry\",\n",
    "        \"strawberries\", \"sugar\", \"syrup\", \"sushi\", \"sweet_potato\", \"taco\", \"tahini\", \"tea\", \"tomato\", \"tomatoes\", \"tofu\", \"tuna\", \"turkey\", \"tiramisu\", \"tortilla\", \"vinegar\", \"waffle\", \"waffles\", \"walnut\", \"walnuts\", \"water\", \"watermelon\", \"watermelons\", \"whipped_cream\", \"yogurt\", \"Yolk\", \"Zucchini\"\n",
    "    ],\n",
    "\n",
    "    \"Emotions\": [\n",
    "    \"afraid\", \"agitated\", \"alienated\", \"amused\", \"angry\", \"annoyed\", \"anxious\", \"apathetic\", \"apprehensive\", \"ashamed\", \"awe-struck\",\n",
    "    \"bewildered\", \"blissful\", \"bored\", \"brave\", \"calm\", \"caring\", \"cheerful\", \"compassionate\", \"confident\", \"confused\", \"cowardly\",\n",
    "    \"curious\", \"delighted\", \"depressed\", \"despairing\", \"determined\", \"discouraged\", \"disappointed\", \"distraught\", \"doubtful\", \n",
    "    \"ecstatic\", \"elated\", \"embarrassed\", \"emotional\", \"energized\", \"enthusiastic\", \"envious\", \"excited\", \"exhausted\", \"exhilarated\", \n",
    "    \"fearful\", \"flustered\", \"frustrated\", \"furious\", \"giddy\", \"gloomy\", \"grateful\", \"grumpy\", \"guilty\", \"happy\", \"heartbroken\", \n",
    "    \"hesitant\", \"hopeful\", \"hopeless\", \"horrified\", \"hostile\", \"humiliated\", \"impatient\", \"in_love\", \"indignant\", \"indifferent\", \n",
    "    \"insecure\", \"irritated\", \"jealous\", \"joyful\", \"jubilant\", \"kind\", \"lazy\", \"lively\", \"lonely\", \"loved\", \"melancholic\", \"meltdown\", \n",
    "    \"miserable\", \"moody\", \"nervous\", \"nostalgic\", \"nurturing\", \"open-hearted\", \"optimistic\", \"overjoyed\", \"overwhelmed\", \n",
    "    \"panicked\", \"panicky\", \"passionate\", \"patient\", \"peaceful\", \"pensive\", \"pessimistic\", \"pleased\", \"proud\", \"quiet\", \"regretful\", \n",
    "    \"relaxed\", \"relieved\", \"resentful\", \"revulsed\", \"romantic\", \"satisfied\", \"scared\", \"sentimental\", \"serene\", \"shaken\", \"shy\", \n",
    "    \"skeptical\", \"smug\", \"stern\", \"strong\", \"stunned\", \"surprised\", \"sympathetic\", \"tearful\", \"tense\", \"terrified\", \"thoughtful\", \n",
    "    \"thrilled\", \"tired\", \"touched\", \"tranquil\", \"trusting\", \"uncomfortable\", \"unhappy\", \"uplifted\", \"upset\", \"vulnerable\", \"wistful\", \n",
    "    \"worried\", \"yearning\"\n",
    "    ],\n",
    "\n",
    "    \"Body Parts\": [\n",
    "        \"abdomen\", \"ankle\", \"ankles\", \"appendix\", \"arm\", \"arms\", \"artery\", \"arteries\", \"belly\", \"belly_button\", \"bladder\", \"blood\", \"bone\", \"bones\", \"brain\",\n",
    "        \"calf\", \"calves\", \"cartilage\", \"cheek\", \"cheeks\", \"chest\", \"chin\", \"ear\", \"ears\", \"eardrum\", \"elbow\", \"elbows\", \"eye\", \"eyes\",\n",
    "        \"eyebrow\", \"eyebrows\", \"eyelash\", \"eyelashes\", \"eyeball\", \"eyeballs\", \"finger\", \"fingers\", \"foot\", \"feet\", \"forehead\", \"gallbladder\", \"genital\", \"genitals\", \"groin\", \"gums\", \"hand\", \"hands\",\n",
    "        \"hair\", \"head\", \"heel\", \"heels\", \"hips\", \"jaw\", \"kidney\", \"kidneys\", \"knee\", \"knees\", \"larynx\", \"leg\", \"legs\", \"ligament\", \"lip\", \"lips\",\n",
    "        \"liver\", \"lung\", \"lungs\", \"mouth\", \"muscle\", \"muscles\", \"nail\", \"nails\", \"navel\", \"neck\", \"nerve\", \"nerves\", \"nostril\", \"nostrils\", \"ovary\", \"ovaries\",\n",
    "        \"palms\", \"pancreas\", \"pelvis\", \"penis\", \"pupil\", \"retina\", \"rib\", \"ribs\", \"scalp\", \"scrotum\", \"shoulder\", \"shoulders\", \"shin\", \"shins\", \"skin\",\n",
    "        \"skull\", \"spine\", \"spleen\", \"stomach\", \"teeth\", \"tendon\", \"thigh\", \"thighs\", \"throat\", \"thumb\", \"thumbs\", \"toe\", \"toes\", \"tongue\", \"tonsils\",\n",
    "        \"tooth\", \"trachea\", \"ureter\", \"urethra\", \"uterus\", \"uvula\", \"vein\", \"veins\", \"vocal_cords\", \"waist\", \"wrist\", \"wrists\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Define multi-word keywords to preserve as single tokens\n",
    "multi_word_keywords = [\n",
    "    \"ice cream\", \"sweet potato\", \"whipped cream\", \"tank top\", \"tank tops\", \"in love\", \"belly button\", \"vocal cords\"\n",
    "]\n",
    "\n",
    "def clean_lyrics(lyrics):\n",
    "    \"\"\"Clean lyrics by converting to lowercase, splitting hyphens, and removing special characters.\"\"\"\n",
    "    lyrics = str(lyrics).lower()  # Convert to string to handle non-string inputs\n",
    "    # Handle possessives (e.g., cat's â†’ cat)\n",
    "    lyrics = re.sub(r'\\b(\\w+)\\'s\\b', r'\\1', lyrics)  # Remove 's\n",
    "    lyrics = re.sub(r'\\b(\\w+)\\'\\b', r'\\1', lyrics)  # Remove standalone '\n",
    "    # lyrics = lyrics.replace('-', ' ')  # Replace hyphens with spaces to split words\n",
    "\n",
    "    # Preserve multi-word keywords by replacing spaces with underscores\n",
    "    for phrase in multi_word_keywords:\n",
    "        lyrics = lyrics.replace(phrase, phrase.replace(' ', '_'))\n",
    "\n",
    "    lyrics = re.sub(r'[^\\w\\s]', '', lyrics)  # Remove special characters except spaces\n",
    "    return lyrics\n",
    "\n",
    "\n",
    "def assign_categories_and_words(lyrics):\n",
    "    \"\"\"Assign categories and track relevant words based on unique word occurrences.\"\"\"\n",
    "    words = set(lyrics.split())  # Use set to get unique words\n",
    "    assigned_categories = []\n",
    "    category_words = []\n",
    "    \n",
    "    for category, keywords in categories.items():\n",
    "        # Find matched keywords, converting underscores to spaces for comparison\n",
    "        matched_words = [word.replace('_', ' ') for word in words if word.replace('_', ' ') in keywords]\n",
    "        # Ensure at least three different words\n",
    "        if len(set(matched_words)) >= 3:\n",
    "            assigned_categories.append(category)\n",
    "            category_words.append(matched_words)\n",
    "    \n",
    "    if not assigned_categories:\n",
    "        return [\"None\"], [\"None\"]\n",
    "    return assigned_categories, category_words\n",
    "\n",
    "def preprocess_dataset(input_file, output_file, lyrics_column='Lyrics'):\n",
    "    \"\"\"Read dataset, process lyrics, assign categories and words, and save to new CSV.\"\"\"\n",
    "    try:\n",
    "        # Read the input CSV\n",
    "        df = pd.read_csv(input_file)\n",
    "        \n",
    "        # Check if required columns exist\n",
    "        required_columns = [lyrics_column, 'Song', 'Artist', 'Genre']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            print(f\"Error: Missing columns {missing_columns} in {input_file}\")\n",
    "            print(\"Available columns:\", df.columns.tolist())\n",
    "            raise KeyError(f\"Missing columns: {missing_columns}\")\n",
    "        \n",
    "        # Clean lyrics\n",
    "        df['cleaned_lyrics'] = df[lyrics_column].apply(clean_lyrics)\n",
    "        \n",
    "        # Assign categories and relevant words\n",
    "        df[['categories', 'category_words']] = df['cleaned_lyrics'].apply(\n",
    "            lambda x: pd.Series(assign_categories_and_words(x))\n",
    "        )\n",
    "        \n",
    "        # Convert category_words to string representation for CSV storage\n",
    "        df['category_words'] = df['category_words'].apply(\n",
    "            lambda x: json.dumps(x) if x != [\"None\"] else \"[]\"\n",
    "        )\n",
    "        \n",
    "        # Select only the requested columns for output\n",
    "        output_columns = ['Song', 'Artist', 'Genre', 'categories', 'category_words', 'cleaned_lyrics']\n",
    "        df = df[output_columns]\n",
    "        \n",
    "        # Save the processed dataset\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Processed dataset saved to {output_file}\")\n",
    "        return df\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {input_file} was not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def train_multi_label_classifier(train_file, test_file, model_output_path, vectorizer_output_path):\n",
    "    \"\"\"Train a multi-label classifier using TF-IDF features and save the model.\"\"\"\n",
    "    # Preprocess training data\n",
    "    print(\"Preprocessing training data...\")\n",
    "    train_df = preprocess_dataset(train_file, \"../data/processed_train_dataset.csv\")\n",
    "    if train_df is None:\n",
    "        return\n",
    "    \n",
    "    # Preprocess test data (assuming already processed, but load for consistency)\n",
    "    print(\"Loading test data...\")\n",
    "    try:\n",
    "        test_df = pd.read_csv(test_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {test_file} was not found.\")\n",
    "        return\n",
    "    \n",
    "    # Prepare labels\n",
    "    mlb = MultiLabelBinarizer(classes=list(categories.keys()))\n",
    "    train_df['categories'] = train_df['categories'].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "    train_df['categories'] = train_df['categories'].apply(lambda x: [] if x == ['None'] else x)\n",
    "    y_train = mlb.fit_transform(train_df['categories'])\n",
    "    \n",
    "    test_df['categories'] = test_df['categories'].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "    test_df['categories'] = test_df['categories'].apply(lambda x: [] if x == ['None'] else x)\n",
    "    y_test = mlb.transform(test_df['categories'])\n",
    "    \n",
    "    # Extract TF-IDF features\n",
    "    vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "    X_train = vectorizer.fit_transform(train_df['cleaned_lyrics'])\n",
    "    X_test = vectorizer.transform(test_df['cleaned_lyrics'])\n",
    "    \n",
    "    # Train the classifier\n",
    "    print(\"Training the classifier...\")\n",
    "    classifier = OneVsRestClassifier(LogisticRegression(max_iter=1000))\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"Evaluating on test set...\")\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    hamming = hamming_loss(y_test, y_pred)\n",
    "    f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    print(f\"Hamming Loss: {hamming:.4f}\")\n",
    "    print(f\"F1 Score (Micro): {f1_micro:.4f}\")\n",
    "    print(f\"F1 Score (Macro): {f1_macro:.4f}\")\n",
    "    \n",
    "    # Save the model and vectorizer\n",
    "    joblib.dump(classifier, model_output_path)\n",
    "    joblib.dump(vectorizer, vectorizer_output_path)\n",
    "    print(f\"Model saved to {model_output_path}\")\n",
    "    print(f\"Vectorizer saved to {vectorizer_output_path}\")\n",
    "    \n",
    "    # Example predictions\n",
    "    print(\"\\nExample predictions on test set:\")\n",
    "    for i in range(min(5, len(test_df))):\n",
    "        song = test_df['Song'].iloc[i]\n",
    "        true_labels = mlb.inverse_transform(y_test[i:i+1])[0]\n",
    "        pred_labels = mlb.inverse_transform(y_pred[i:i+1])[0]\n",
    "        print(f\"Song: {song}\")\n",
    "        print(f\"True Categories: {true_labels}\")\n",
    "        print(f\"Predicted Categories: {pred_labels}\")\n",
    "        print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_file = \"../data/train.csv\"\n",
    "    test_file = \"../data/processed_test_dataset.csv\"\n",
    "    model_output_path = \"../models/multi_label_classifier.pkl\"\n",
    "    vectorizer_output_path = \"../models/tfidf_vectorizer.pkl\"\n",
    "    train_multi_label_classifier(train_file, test_file, model_output_path, vectorizer_output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
