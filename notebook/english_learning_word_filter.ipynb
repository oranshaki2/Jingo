{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b671afc",
   "metadata": {},
   "source": [
    "# English Learning App - Word Sense Filtering\n",
    "This notebook filters words from song lyrics that don't match their intended semantic meaning based on translation into Hebrew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65317658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hila/Jingo-1/jupyter-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✔️ HATE found via direct match: 'שנאה' in 'זה מתחיל בכאב ואחריו שנאה'\n",
      "\n",
      "✔️ SHAME found via direct match: 'בושה' in 'האם אתה שפוי איפה הבושה'\n",
      "\n",
      "✔️ FEELING found via direct match: 'תחושה' in 'זו תחושה שאתה מקבל כשאתה לא יכול למצוא את הדרך שלך'\n",
      "\n",
      "Word: kind\n",
      "Original Sentence: what kind of world do we live in\n",
      "Translated (HE): באיזה סוג עולם אנו חיים\n",
      "Intended (HE): אדיב\n",
      "Similarity: 0.7474 — ❌ SKIP\n",
      "\n",
      "✔️ SANE found via direct match: 'שפוי' in 'האם אתה שפוי איפה הבושה'\n",
      "\n",
      "✔️ LOVE found via direct match: 'אהבה' in 'שם האהבה מחולקת בשנאה'\n",
      "\n",
      "✔️ SHAME found via direct match: 'בושה' in 'נמאס מכל הבושה שהרגשתי'\n",
      "\n",
      "✔️ DOUBT found via direct match: 'ספק' in 'לעולם לא יהיה ספק'\n",
      "\n",
      "✔️ TIRED OF found via direct match: 'נמאס' in 'נמאס מכל הבושה שהרגשתי'\n",
      "\n",
      "Word: loved\n",
      "Original Sentence: but you loved me anyway\n",
      "Translated (HE): אבל בכל מקרה אהבת אותי\n",
      "Intended (HE): אהוב\n",
      "Similarity: 0.8470 — ✔️ KEEP\n",
      "\n",
      "✔️ SICK found via direct match: 'חולה' in 'הייתי חולה מכל הכאב'\n",
      "\n",
      "✔️ LOVE found via direct match: 'אהבה' in 'אהבה שונאת גאווה'\n",
      "\n",
      "Word: hate\n",
      "Original Sentence: love hate pride\n",
      "Translated (HE): אהבה שונאת גאווה\n",
      "Intended (HE): שנאה\n",
      "Similarity: 0.7613 — ✔️ KEEP\n",
      "\n",
      "Word: feeling\n",
      "Original Sentence: im feeling superhuman\n",
      "Translated (HE): אני מרגיש על -אנושי\n",
      "Intended (HE): תחושה\n",
      "Similarity: 0.8151 — ✔️ KEEP\n",
      "\n",
      "✔️ BLOOD found via direct match: 'דם' in 'הדם נמצא במים'\n",
      "\n",
      "✔️ FINGER found via direct match: 'אצבע' in 'אתה לא מכוון את האצבע'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from googletrans import Translator\n",
    "import time\n",
    "from difflib import SequenceMatcher\n",
    "from category_in_hebrew import intended_hebrew\n",
    "\n",
    "# --- General Configuration ---\n",
    "BATCH_SIZE = 1000\n",
    "INPUT_PATH = '../data/songs_category_before_tran.csv'\n",
    "OUTPUT_PATH = '../data/filtered_songs_disambiguated.csv'\n",
    "\n",
    "# --- Load model and translation tools ---\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "translator = Translator()\n",
    "translation_cache = {}\n",
    "\n",
    "# Retry mechanism for Google Translate to handle rate limits/errors\n",
    "def translate_with_retry(sentence, retries=3, delay=1):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            return translator.translate(sentence, src='en', dest='he').text\n",
    "        except Exception as e:\n",
    "            print(f\"Translation failed (attempt {attempt+1}): {e}\")\n",
    "            time.sleep(delay)\n",
    "    return None\n",
    "\n",
    "# Use cache to avoid redundant translations\n",
    "def translate_with_cache(sentence):\n",
    "    if sentence in translation_cache:\n",
    "        return translation_cache[sentence]\n",
    "    translated = translate_with_retry(sentence)\n",
    "    if translated:\n",
    "        translation_cache[sentence] = translated\n",
    "    return translated\n",
    "\n",
    "# Basic sentence splitter\n",
    "def split_sentences(text):\n",
    "    return re.split(r'[.!?\\n]', text)\n",
    "\n",
    "# Check if the word is used in the intended sense\n",
    "def is_correct_sense(word, sentence, intended_hebrew_meaning, threshold=0.75):\n",
    "    translated = translate_with_cache(sentence)\n",
    "    if not translated:\n",
    "        return False\n",
    "\n",
    "    # Direct match\n",
    "    if intended_hebrew_meaning in translated:\n",
    "        print(f\"\\n✔️ {word.upper()} found via direct match: '{intended_hebrew_meaning}' in '{translated}'\")\n",
    "        return True\n",
    "\n",
    "    # Fuzzy string match\n",
    "    similarity_str = SequenceMatcher(None, translated, intended_hebrew_meaning).ratio()\n",
    "    if similarity_str > 0.85:\n",
    "        print(f\"\\n✔️ {word.upper()} found via fuzzy string match ({similarity_str:.2f})\")\n",
    "        return True\n",
    "\n",
    "    # Semantic similarity using sentence embeddings\n",
    "    try:\n",
    "        emb_sentence = model.encode(translated, convert_to_tensor=True)\n",
    "        emb_meaning = model.encode(intended_hebrew_meaning, convert_to_tensor=True)\n",
    "        similarity = util.pytorch_cos_sim(emb_sentence, emb_meaning).item()\n",
    "        print(f\"\\nWord: {word}\")\n",
    "        print(f\"Original Sentence: {sentence}\")\n",
    "        print(f\"Translated (HE): {translated}\")\n",
    "        print(f\"Intended (HE): {intended_hebrew_meaning}\")\n",
    "        print(f\"Similarity: {similarity:.4f} — {'✔️ KEEP' if similarity >= threshold else '❌ SKIP'}\")\n",
    "        return similarity >= threshold\n",
    "    except Exception as e:\n",
    "        print(f\"Embedding failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# --- Load data ---\n",
    "df = pd.read_csv(INPUT_PATH)\n",
    "\n",
    "# Load previously processed data if exists\n",
    "if os.path.exists(OUTPUT_PATH):\n",
    "    existing_df = pd.read_csv(OUTPUT_PATH)\n",
    "    processed_count = len(existing_df)\n",
    "else:\n",
    "    existing_df = pd.DataFrame()\n",
    "    processed_count = 0\n",
    "\n",
    "print(f\"Processed so far: {processed_count}\")\n",
    "\n",
    "# Select the next batch of songs to process\n",
    "df_batch = df.iloc[processed_count:processed_count + BATCH_SIZE]\n",
    "\n",
    "if df_batch.empty:\n",
    "    print(\"✅ All songs processed.\")\n",
    "    exit()\n",
    "\n",
    "filtered_rows = []\n",
    "\n",
    "# --- Process current batch ---\n",
    "for idx, row in df_batch.iterrows():\n",
    "    lyrics = row['cleaned_lyrics'].lower()\n",
    "    updated_groups = []\n",
    "    category_groups = ast.literal_eval(row['category_words'])\n",
    "    sentences = split_sentences(lyrics)\n",
    "\n",
    "    for group in category_groups:\n",
    "        filtered_group = []\n",
    "        for word in group:\n",
    "            found = False\n",
    "            for category_dict in intended_hebrew.values():\n",
    "                if word in category_dict:\n",
    "                    intended_hebrew_meaning = category_dict[word]\n",
    "                    sentence = next((s.strip() for s in sentences if word in s), None)\n",
    "                    if sentence and is_correct_sense(word, sentence, intended_hebrew_meaning):\n",
    "                        filtered_group.append(word)\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                filtered_group.append(word)\n",
    "\n",
    "        if filtered_group:\n",
    "            updated_groups.append(filtered_group)\n",
    "\n",
    "    row['category_words'] = str(updated_groups)\n",
    "    filtered_rows.append(row)\n",
    "\n",
    "# --- Save results ---\n",
    "filtered_df = pd.DataFrame(filtered_rows)\n",
    "combined_df = pd.concat([existing_df, filtered_df], ignore_index=True)\n",
    "combined_df.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(f\"✅ Batch processed and saved: {processed_count} ➡ {processed_count + len(filtered_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83b437ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total songs in dataset: ../data/filtered_songs_three.csv: 96515\n"
     ]
    }
   ],
   "source": [
    "input_file = '../data/filtered_songs_three.csv'\n",
    "df = pd.read_csv(input_file)\n",
    "print(f\"Total songs in dataset: {input_file}: {len(df)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
